==============================================================================
sllm.nvim docs                                                       *sllm-docs*

This folder contains the Markdown sources for the plugin documentation.

Getting started (defaults)

1. Install the `llm` CLI (e.g., `brew install llm` or `pip install llm`).
2. Install an extension, for example `llm install llm-openrouter` or
   `llm install llm-anthropic`.
3. Set your API key with `llm keys set <provider>` (or an environment
   variable).
4. Install the plugin with your manager (lazy.nvim example):

>lua
    {
      'mozanunal/sllm.nvim',
      dependencies = {
        'echasnovski/mini.notify',
        'echasnovski/mini.pick',
      },
      config = function()
        require('sllm').setup()
      end,
    }
<
The defaults ship with streaming chat, inline completion, history browsing,
context capture, and slash commands. Keymaps start with `<leader>sâ€¦` (ask,
model, mode, context, commands, history, toggle, cancel, complete).

More guides

- Configuration and defaults: `doc/configure.md`
- Slash commands reference: `doc/slash_commands.md`
- Modes and templates: `doc/modes.md`
- Hooks (pre/post): `doc/hooks.md`
- LLM backend setup: `doc/backend_llm.md`

==============================================================================
Configure sllm.nvim                                             *sllm-configure*

This guide shows the default configuration and how to change keymaps or add
slash commands.

Default configuration

Call `setup()` with no arguments to use the defaults:

>lua
    require('sllm').setup({
      backend_config = { cmd = 'llm' },
      default_model = 'default',
      default_mode = 'sllm_chat',
      on_start_new_chat = true,
      reset_ctx_each_prompt = true,
      window_type = 'vertical',
      scroll_to_bottom = true,
      pick_func = (pcall(require, 'mini.pick') and require('mini.pick').ui_select) or vim.ui.select,
      notify_func = (pcall(require, 'mini.notify') and require('mini.notify').make_notify()) or vim.notify,
      input_func = vim.ui.input,
      pre_hooks = nil,
      post_hooks = nil,
      history_max_entries = 1000,
      chain_limit = 100,
      keymaps = {
        ask = '<leader>ss',
        select_model = '<leader>sm',
        select_mode = '<leader>sM',
        add_context = '<leader>sa',
        commands = '<leader>sx',
        new_chat = '<leader>sn',
        cancel = '<leader>sc',
        toggle_buffer = '<leader>st',
        history = '<leader>sh',
        copy_code = '<leader>sy',
        complete = '<leader><Tab>',
      },
      ui = {
        show_usage = true,
        ask_llm_prompt = 'Prompt: ',
        add_url_prompt = 'URL: ',
        add_cmd_prompt = 'Command: ',
        markdown_prompt_header = '> ðŸ’¬ Prompt:',
        markdown_response_header = '> ðŸ¤– Response',
        set_system_prompt = 'System Prompt: ',
      },
    })
<
Customizing keymaps

- Omit `keymaps` to keep the defaults.
- Set a specific key to a new mapping: `ask = '<leader>a'`.
- Disable one by setting it to `false` or `nil`.
- Disable all defaults by setting `keymaps = false`, then define your own
  mappings that call functions like `require('sllm').ask_llm()` or
  `require('sllm').commands`.

Adjusting UI and behavior

- Switch window type with `window_type = 'horizontal'` or `'float'`.
- Keep context across prompts by setting `reset_ctx_each_prompt = false`.
- Start with online mode enabled via `online_enabled = true`.
- Change prompts and headers in the `ui` table (see defaults above).

Adding slash commands to your config

Slash commands are handled by `Sllm.run_command` and map to the internal
command registry. Today the registry is fixed inside the plugin. If you need
custom slash commands, the supported approach is to add your own keymaps that
call your functions directly (for example,
`vim.keymap.set({ 'n', 'v' }, '<leader>sx', my_fn)`) or open a PR to expose a
public extension point.

Key hints

- `default_model = 'default'` respects the model configured in the `llm` CLI.
- `history_max_entries` controls how many conversations are fetched in the
  history picker.
- `chain_limit` caps how many tool calls the agent can make in a single
  interaction.

==============================================================================
Slash commands                                                      *sllm-slash*

Type `/` at the prompt to open the command picker, or type `/command` directly
to run one. Commands are grouped by purpose.

Chat

- `/new` starts a new chat and clears the buffer.
- `/history` opens the conversation picker (uses `history_max_entries`).
- `/cancel` stops the current request.

Context

- `/file` adds the current buffer path to context.
- `/url` prompts for a URL and adds its content.
- `/selection` adds the current visual selection.
- `/diagnostics` adds the current buffer diagnostics.
- `/command` runs a shell command and adds its output.
- `/tool` adds an installed llm tool.
- `/function` adds a Python function as a tool.
- `/clear` clears all context (files, snippets, tools, functions).

Model and modes

- `/model` picks a model.
- `/mode` picks a template/mode.
- `/online` toggles online/web mode.
- `/options` shows `llm models --options` for the active model.
- `/system` sets the system prompt.
- `/option` sets a model option key/value.
- `/reset-options` clears all model options.

Templates

- `/template` shows the active template contents in a buffer.
- `/edit` opens the active template for editing.

Copy and UI

- `/code` copies the last code block from the response buffer.
- `/code-first` copies the first code block.
- `/response` copies the last response.
- `/focus` focuses the LLM window.
- `/toggle` toggles the LLM window.

==============================================================================
Modes and templates                                                 *sllm-modes*

sllm.nvim treats `llm` templates as modes. The plugin ships with four defaults
that are symlinked into your `llm` template directory on setup:

- `sllm_chat` for general chat without tools.
- `sllm_read` for review/read-only operations.
- `sllm_agent` for tool-assisted, agentic workflows (bash/edit/write).
- `sllm_complete` for inline completion (`<leader><Tab>`).

Switching modes

- Use the mode picker: `<leader>sM` or `/mode`.
- The current mode is shown in the winbar alongside the model.

Customizing templates

Templates are standard `llm` YAML files. Common tasks:

- Edit an existing template: `llm templates edit sllm_agent`.
- Duplicate and modify: copy a shipped template to a new name under
  `~/.config/io.datasette.llm/templates/` and edit it.
- View contents: `llm templates show sllm_agent`.

Custom templates appear in the picker automatically and can be set as
`default_mode` in your config.

==============================================================================
Hooks                                                               *sllm-hooks*

Hooks let you run shell commands before or after an LLM request.

Pre-hooks

- Run before the LLM starts.
- Optional `add_to_context` flag captures stdout/stderr into the context.

Example:

>lua
    require('sllm').setup({
      pre_hooks = {
        { command = 'git diff --cached', add_to_context = true },
        { command = 'echo Starting request', add_to_context = false },
      },
    })
<
Notes:

- Output is stored as a snippet with the command name.
- Snippets are cleared after each prompt if `reset_ctx_each_prompt = true`.
- Hooks run synchronously in order.

Post-hooks

- Run after the response finishes (success or failure).
- Output is not captured; use for logging or notifications.

Example:

>lua
    require('sllm').setup({
      post_hooks = {
        { command = 'date >> ~/.sllm_history.log' },
        { command = "osascript -e 'display notification \"LLM request completed\" with title \"SLLM\"'" },
      },
    })
<
Tips:

- Use `%` in commands to expand to the current file path.
- Keep commands fast to avoid blocking the response finish.

==============================================================================
LLM backend setup                                                 *sllm-backend*

sllm.nvim wraps the `llm` CLI. Set up the backend first, then use the plugin.

Install llm

- Homebrew: `brew install llm`
- pipx: `pipx install llm`
- pip: `pip install llm`

Verify with `llm --help`.

Install extensions

Install at least one provider plugin:

- `llm install llm-openrouter` for many models via OpenRouter.
- `llm install llm-anthropic` for Claude models.
- `llm install llm-openai` for OpenAI models.
- `llm install llm-gpt4all` for local models.

Browse more at https://llm.datasette.io/en/stable/plugins/directory.html.

Set API keys

Use `llm keys set <provider>` (e.g., `llm keys set openrouter`) or environment
variables like `OPENAI_API_KEY`.

Useful tool extensions

- `llm install llm-quickjs` for JavaScript tools.
- `llm install llm-shell` for shell execution (paired with agent mode).
- `llm install llm-curl` for HTTP fetch helpers.

Check each plugin's README for permissions and configuration.

Configure sllm.nvim

In `setup()`, point to your llm binary if needed:
`backend_config = { cmd = '/full/path/to/llm' }`. The default uses `llm` from
`PATH`.

If the llm templates directory is non-standard, ensure your `llm` install is
reachable so the plugin can symlink its templates on setup.

 vim:tw=78:ts=2:sw=2:et:ft=help:norl:
