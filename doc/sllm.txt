                                                                     *sllm.nvim*
                                                                          *sllm*
*sllm.nvim* Integrate Simon Willison's llm CLI into Neovim

MIT License Copyright (c) 2025 mozanunal

                                                                        *Sllm*

Introduction

# Introduction

sllm.nvim is a Neovim plugin that integrates Simon Willison's `llm` CLI
directly into your editor. Chat with large language models, stream responses
in a scratch buffer, manage context files, switch models or tool integrations
on the fly, and control everything asynchronously without leaving Neovim.

Features:
  â€¢ Interactive chat with streaming responses
  â€¢ Code completion at cursor position
  â€¢ History navigation (browse and continue conversations)
  â€¢ Context management (files, URLs, selections, diagnostics, etc.)
  â€¢ Model and tool selection
  â€¢ On-the-fly Python function tools
  â€¢ Asynchronous, non-blocking requests
  â€¢ Split buffer UI with markdown rendering
  â€¢ Token usage feedback
  â€¢ Code block extraction

Requirements

# Requirements

1. The `llm` CLI must be installed:
   https://github.com/simonw/llm

2. At least one `llm` extension (e.g., `llm install llm-openai`)

3. Configure API keys (e.g., `llm keys set openai`)

Installation

# Installation

Using lazy.nvim: >lua
  {
    "mozanunal/sllm.nvim",
    dependencies = {
      "echasnovski/mini.notify",  -- optional
      "echasnovski/mini.pick",    -- optional
    },
    config = function()
      require("sllm").setup({
        -- your custom options here
      })
    end,
  }
<

Configuration

# Configuration ~

All configuration options with their defaults: >lua
  require("sllm").setup({
    llm_cmd = "llm",            -- Command or path for the llm CLI
    default_model = "default",  -- Model to use (or "default" for llm's default)
    show_usage = true,          -- Show token usage stats after responses
    on_start_new_chat = true,   -- Start with fresh chat on setup
    reset_ctx_each_prompt = true, -- Clear context after each prompt
    window_type = "vertical",   -- "vertical", "horizontal", or "float"
    scroll_to_bottom = true,    -- Auto-scroll to bottom of LLM window
    pick_func = vim.ui.select,  -- Function for item selection
    notify_func = vim.notify,   -- Function for notifications
    input_func = vim.ui.input,  -- Function for input prompts
    keymaps = { ... },          -- See |Sllm-keymaps|
    pre_hooks = nil,            -- Commands before LLM execution
    post_hooks = nil,           -- Commands after LLM execution
    system_prompt = "...",      -- System prompt for all queries
    model_options = {},         -- Model-specific options (-o flags)
    online_enabled = false,     -- Enable web search by default
    history_max_entries = 1000, -- Max history entries to fetch
    ui = {...},                 -- See |Sllm-uiconfig|
  })
<
## Configuration Options ~

`llm_cmd` - Command or full path to the `llm` CLI executable.

`default_model` - Model to use on startup. Set to "default" to use the
default model configured in `llm`.

`show_usage` - When `true`, displays token usage and estimated cost after
each response.

`on_start_new_chat` - When `true`, starts with a fresh chat buffer on setup.

`reset_ctx_each_prompt` - When `true`, automatically clears file context
after each prompt. Set to `false` to persist context across prompts.

`window_type` - Controls how the LLM buffer opens:
  â€¢ "vertical" - Split vertically
  â€¢ "horizontal" - Split horizontally
  â€¢ "float" - Floating window

`scroll_to_bottom` - When `true`, keeps cursor at bottom of LLM window as
responses stream in.

`pick_func`, `notify_func`, `input_func` - UI functions for selections,
notifications, and input prompts. Defaults to vim.ui.* functions but can
use mini.pick and mini.notify for enhanced UI.

`keymaps` - Table of keybindings. See |Sllm-keymaps|. Set to `false` to
disable all default keymaps.

`system_prompt` - Text prepended to all queries via `-s` flag. Useful for
ensuring consistent output formatting. See |Sllm-system-prompt|.

`model_options` - Table of model-specific options passed via `-o` flags.
Example: `{ temperature = 0.7, max_tokens = 1000 }`. See |Sllm-model-options|.

`online_enabled` - When `true`, enables web search capabilities (shows ðŸŒ
in status bar). Not all models support this.

`history_max_entries` - Maximum number of conversation history entries to
fetch. Higher values show more history but may be slower.

`ui` - Table of UI elements (prompts and headers). See |Sllm-uiconfig|.

Keymaps

                                                                *Sllm-keymaps*
# Keymaps ~

Default keybindings (all can be customized or disabled):

Keymap                  | Default Key   | Modes | Description
`ask_llm`               | `<leader>ss`  | n,v   | Prompt the LLM
`new_chat`              | `<leader>sn`  | n,v   | Start new chat
`cancel`                | `<leader>sc`  | n,v   | Cancel current request
`focus_llm_buffer`      | `<leader>sf`  | n,v   | Focus LLM buffer
`toggle_llm_buffer`     | `<leader>st`  | n,v   | Toggle LLM buffer
`select_model`          | `<leader>sm`  | n,v   | Select model
`toggle_online`         | `<leader>sW`  | n,v   | Toggle online/web mode
`set_model_option`      | `<leader>so`  | n,v   | Set model option
`show_model_options`    | `<leader>sO`  | n,v   | Show model options
`add_file_to_ctx`       | `<leader>sa`  | n,v   | Add current file
`add_url_to_ctx`        | `<leader>su`  | n,v   | Add URL content
`add_sel_to_ctx`        | `<leader>sv`  | v     | Add visual selection
`add_diag_to_ctx`       | `<leader>sd`  | n,v   | Add diagnostics
`add_cmd_out_to_ctx`    | `<leader>sx`  | n,v   | Add command output
`add_tool_to_ctx`       | `<leader>sT`  | n,v   | Add tool
`add_func_to_ctx`       | `<leader>sF`  | n,v   | Add Python function
`reset_context`         | `<leader>sr`  | n,v   | Reset context
`set_system_prompt`     | `<leader>sS`  | n,v   | Set system prompt
`browse_history`        | `<leader>sh`  | n,v   | Browse history
`copy_last_code_block`  | `<leader>sy`  | n,v   | Copy last code block
`copy_first_code_block` | `<leader>sY`  | n,v   | Copy first code block
`copy_last_response`    | `<leader>sE`  | n,v   | Copy last response
`complete_code`         | `<leader><Tab>` | n,v | Complete code at cursor

## Customizing Keymaps ~

Change specific keymaps: >lua
  require("sllm").setup({
    keymaps = {
      ask_llm = "<leader>a",    -- Change to <leader>a
      add_url_to_ctx = false,   -- Disable this keymap
    },
  })
<
Disable all default keymaps: >lua
  require("sllm").setup({
    keymaps = false,
  })

  -- Then define your own
  vim.keymap.set("n", "<leader>a", require("sllm").ask_llm)
<

Usage

# Usage ~

## Basic Workflow ~

1. Press `<leader>ss` - Ask the LLM a question
2. Press `<leader>sa` - Add current file to context
3. Press `<leader>sv` (visual mode) - Add selection to context
4. Press `<leader>sm` - Switch models
5. Press `<leader>sh` - Browse and continue previous conversations
6. Press `<leader><Tab>` - Complete code at cursor position

## Code Completion ~

The code completion feature (`<leader><Tab>`) sends code before and after
your cursor to the LLM for intelligent completion:

1. Position cursor where you want completion
2. Press `<leader><Tab>`
3. LLM analyzes context and inserts completion

## Context Management ~

Build context for better LLM responses:

Context is automatically cleared after each prompt by default
(configurable with `reset_ctx_each_prompt`).

## History Navigation ~

Press `<leader>sh` to browse previous conversations:

The `llm` CLI logs all interactions automatically. You can manage logs
with: `llm logs list`, `llm logs off`, etc.

System Prompt

                                                         *Sllm-system-prompt*
# System Prompt ~

The system prompt is prepended to all queries using the `-s` flag. This
ensures consistent behavior and output formatting.

## Default System Prompt ~
>
  You are a sllm plugin living within neovim.
  Always answer with markdown.
  If the offered change is small, return only the changed part or
  function, not the entire file.
<
## Configure in setup() ~
>lua
  require("sllm").setup({
    system_prompt = [[You are an expert code reviewer.
  Always provide constructive feedback.
  Format code suggestions using markdown code blocks.]],
  })
<
## Update on-the-fly ~

Press `<leader>sS` to interactively update the system prompt during a
session. This allows you to adapt the LLM's behavior without restarting
Neovim. Submit an empty string to clear the system prompt.

Model Options

                                                         *Sllm-model-options*
# Model Options ~

Models support specific options passed via `-o` flags. Common options
include temperature, max_tokens, and more.

## Discover Options ~

Press `<leader>sO` (capital O) to see available options for your current
model, or run: `llm models --options -m <model-name>`

## Set Options in Config ~
>lua
  require("sllm").setup({
    model_options = {
      temperature = 0.7,     -- Control randomness (0-2)
      max_tokens = 1000,     -- Limit response length
      top_p = 0.9,          -- Nucleus sampling
      seed = 42,            -- Deterministic sampling
    },
  })
<
## Set Options at Runtime ~

Press `<leader>so` to set an option on-the-fly:
1. Enter option key (e.g., `temperature`)
2. Enter option value (e.g., `0.7`)

Or use Lua: >lua
  require("sllm").set_model_option()
  require("sllm").reset_model_options()  -- Clear all options
<
## Common Options ~


Not all options are available for all models.

Pre/Post Hooks

                                                              *Sllm-hooks*
# Pre-Hooks and Post-Hooks ~

Hooks allow running shell commands before and after LLM execution.

## Pre-Hooks ~

Run before LLM invocation. Can capture output and add to context: >lua
  require("sllm").setup({
    pre_hooks = {
      {
        command = "git diff --cached",
        add_to_context = true,  -- Capture and add to context
      },
      {
        command = "echo 'Starting LLM...'",
        add_to_context = false,  -- Just run, don't capture
      },
    },
  })
<
Pre-hook output is added as a snippet labeled `Pre-hook-> <command>`.

## Post-Hooks ~

Run after LLM completes (both on success and failure): >lua
  require("sllm").setup({
    post_hooks = {
      {
        command = "date >> ~/.sllm_history.log",
      },
      {
        command = "notify-send 'SLLM' 'Request completed'",
      },
    },
  })
<
Post-hook output is not captured or displayed.

## Use Cases ~


Online Mode

                                                           *Sllm-online-mode*
# Online/Web Mode ~

Some models support an `online` option for web search capabilities.

## Toggle Online Mode ~

Press `<leader>sW` to toggle online mode. When enabled, you'll see a ðŸŒ
icon in the status bar.

Status bar examples:
  `sllm.nvim | Model: gpt-4o ðŸŒ`    (online mode enabled)
  `sllm.nvim | Model: gpt-4o`       (online mode disabled)

## Enable by Default ~
>lua
  require("sllm").setup({
    online_enabled = true,
  })
<
Note: Not all models support online mode. Check your model provider's
documentation.

UIConfig

                                                                *Sllm-uiconfig*
# UI configuration ~

Prompts and headers shown to the user through the Sllm UI. These defaults
can be overriden:
>lua
  ui = {
    ask_llm_prompt = 'Prompt: ',
    add_url_prompt = 'URL: ',
    add_cmd_prompt = 'Command: ',
    markdown_prompt_header = '> ðŸ’¬ Prompt:',
    markdown_response_header = '> ðŸ¤– Response',
    set_system_prompt = 'System Prompt: ',
    -- Note: markdown headers are used in both live chat and history
  }
<

Class ~
{SllmKeymaps}
Fields ~
{ask_llm} `(string|false|nil)`             Keymap for asking the LLM.
{new_chat} `(string|false|nil)`            Keymap for starting a new chat.
{cancel} `(string|false|nil)`              Keymap for canceling a request.
{focus_llm_buffer} `(string|false|nil)`    Keymap for focusing the LLM window.
{toggle_llm_buffer} `(string|false|nil)`   Keymap for toggling the LLM window.
{select_model} `(string|false|nil)`        Keymap for selecting an LLM model.
{add_file_to_ctx} `(string|false|nil)`     Keymap for adding current file to context.
{add_url_to_ctx} `(string|false|nil)`      Keymap for adding a URL to context.
{add_sel_to_ctx} `(string|false|nil)`      Keymap for adding visual selection.
{add_diag_to_ctx} `(string|false|nil)`     Keymap for adding diagnostics.
{add_cmd_out_to_ctx} `(string|false|nil)`  Keymap for adding command output.
{add_tool_to_ctx} `(string|false|nil)`     Keymap for adding a tool.
{add_func_to_ctx} `(string|false|nil)`     Keymap for adding a function.
{reset_context} `(string|false|nil)`       Keymap for resetting the context.
{set_system_prompt} `(string|false|nil)`   Keymap for setting the system prompt.
{set_model_option} `(string|false|nil)`    Keymap for setting model options.
{show_model_options} `(string|false|nil)`  Keymap for showing available model options.
{toggle_online} `(string|false|nil)`       Keymap for toggling online mode.
{copy_first_code_block} `(string|false|nil)`  Keymap for copying the first code block.
{copy_last_code_block} `(string|false|nil)`   Keymap for copying the last code block.
{copy_last_response} `(string|false|nil)`     Keymap for copying the last response.
{complete_code} `(string|false|nil)`          Keymap for triggering code completion at cursor.
{browse_history} `(string|false|nil)`         Keymap for browsing chat history.

Class ~
{PreHook}
Fields ~
{command} `(string)`                     Shell command to execute.
{add_to_context} `(boolean?)`            Whether to capture stdout and add to context (default: false).

Class ~
{PostHook}
Fields ~
{command} `(string)`                     Shell command to execute.

Class ~
{SllmConfig}
Fields ~
{llm_cmd} `(string)`                     Command to run the LLM CLI.
{default_model} `(string)`               Default model name or `"default"`.
{show_usage} `(boolean)`                 Show usage examples flag.
{on_start_new_chat} `(boolean)`          Whether to reset conversation on start.
{reset_ctx_each_prompt} `(boolean)`      Whether to clear context after each prompt.
{window_type} "'`(vertical)`'"|"'horizontal'"|"'float'"  How to open the chat window.
{scroll_to_bottom} `(boolean)`           Whether to keep the cursor at the bottom of the LLM window.
{pick_func} `(fun(items: any[], opts: table?, on_choice: fun(item: any, idx?: integer)))`  Selector UI.
{notify_func} `(fun(msg: string, level?: number))`      Notification function.
{input_func} `(fun(opts: table, on_confirm: fun(input: string?)))`  Input prompt function.
{keymaps} `(SllmKeymaps|false|nil)`      Collection of keybindings.
{pre_hooks} `(PreHook[]?)`               Commands to run before llm execution.
{post_hooks} `(PostHook[]?)`             Commands to run after llm execution.
{system_prompt} `(string?)`              System prompt to prepend to all queries.
{model_options} `(table<string,any>?)`   Model-specific options to pass with -o flag.
{online_enabled} `(boolean?)`            Enable online/web mode by default.
{history_max_entries} `(integer?)`       Maximum number of history entries to fetch (default: 1000).
{ui} `(SllmUIConfig|nil)`                Prompts and text used in the UI.

Class ~
{SllmUIConfig}
{ask_llm_prompt} `(string)`              Prompt displayed by the ask_llm function
{add_url_prompt} `(string)`              Prompt displayed by the add_url_to_ctx function
{add_cmd_prompt} `(string)`              Prompt displayed by the add_cmd_out_to_ctx function
{markdown_prompt_header} `(string)`      Text displayed above the user prompt
{markdown_response_header} `(string)`    Text displayed above the LLM response
{set_system_prompt} `(string)`           Prompt displayed when modifying the system prompt

                                                                  *Sllm.setup()*
                             `Sllm.setup`({config})
Module setup

Parameters ~
{config} `(table|nil)` Module config table. See |Sllm.config|.

Usage ~
>lua
  require('sllm').setup() -- use default config
  -- OR
  require('sllm').setup({}) -- replace {} with your config table
<
                                                                   *Sllm.config*
                                 `Sllm.config`
Defaults ~
>lua
  Sllm.config = vim.deepcopy(H.default_config)
<
                                                                *sllm.ask_llm()*
                                      *Ask* *the* *LLM* *with* *a* *prompt* *from* *the* *user.*

         *Prompt* *the* *LLM* *with* *user* *input.* *If* *in* *visual* *mode,* *automatically* *adds*
                                    *the* *selection* *to* *context* *before* *prompting.*

                                `Sllm.ask_llm`()
Return ~
`(nil)`

                                                                 *Sllm.cancel()*
                                `Sllm.cancel`()
Cancel the in-flight LLM request, if any.
Return ~
`(nil)`

                                                               *Sllm.new_chat()*
                               `Sllm.new_chat`()
Start a new chat (clears buffer and state).
Return ~
`(nil)`

                                                       *Sllm.focus_llm_buffer()*
                           `Sllm.focus_llm_buffer`()
Focus the existing LLM window or create it.
Return ~
`(nil)`

                                                      *Sllm.toggle_llm_buffer()*
                           `Sllm.toggle_llm_buffer`()
Toggle visibility of the LLM window.
Return ~
`(nil)`

                                                           *Sllm.select_model()*
                             `Sllm.select_model`()
Prompt user to select an LLM model.
Return ~
`(nil)`

                                                        *Sllm.add_tool_to_ctx()*
                            `Sllm.add_tool_to_ctx`()
Add a tool to the current context.
Return ~
`(nil)`

                                                        *Sllm.add_file_to_ctx()*
                            `Sllm.add_file_to_ctx`()
Add the current file (or URL) path to the context.
Return ~
`(nil)`

                                                         *Sllm.add_url_to_ctx()*
                            `Sllm.add_url_to_ctx`()
Prompt user for a URL and add it to context.
Return ~
`(nil)`

                                                        *Sllm.add_func_to_ctx()*
                            `Sllm.add_func_to_ctx`()
Add the current function or entire buffer to context.
Return ~
`(nil)`

                                                         *Sllm.add_sel_to_ctx()*
                            `Sllm.add_sel_to_ctx`()
Add the current visual selection as a code snippet.
Return ~
`(nil)`

                                                        *Sllm.add_diag_to_ctx()*
                            `Sllm.add_diag_to_ctx`()
Add current buffer diagnostics to context as a snippet.
Return ~
`(nil)`

                                                     *Sllm.add_cmd_out_to_ctx()*
                          `Sllm.add_cmd_out_to_ctx`()
Prompt for a shell command, run it, and add its output to context.
Return ~
`(nil)`

                                                          *Sllm.reset_context()*
                             `Sllm.reset_context`()
Reset the LLM context (fragments, snippets, tools, functions).
Return ~
`(nil)`

                                                      *Sllm.set_system_prompt()*
                           `Sllm.set_system_prompt`()
Set the system prompt on-the-fly.
Return ~
`(nil)`

                                                     *Sllm.show_model_options()*
                          `Sllm.show_model_options`()
Show available options for the current model.
Return ~
`(nil)`

                                                       *Sllm.set_model_option()*
                           `Sllm.set_model_option`()
Set or update a model option.
Return ~
`(nil)`

                                                    *Sllm.reset_model_options()*
                          `Sllm.reset_model_options`()
Reset all model options.
Return ~
`(nil)`

                                                          *Sllm.toggle_online()*
                             `Sllm.toggle_online`()
Toggle the online feature (adds/removes online=1 option).
Return ~
`(nil)`

                                                      *Sllm.is_online_enabled()*
                           `Sllm.is_online_enabled`()
Get online status for UI display.
Return ~
`(boolean)`

                                                  *Sllm.copy_first_code_block()*
                         `Sllm.copy_first_code_block`()
Copy the first code block from the LLM buffer to the clipboard.
Return ~
`(nil)`

                                                   *Sllm.copy_last_code_block()*
                         `Sllm.copy_last_code_block`()
Copy the last code block from the LLM buffer to the clipboard.
Return ~
`(nil)`

                                                     *Sllm.copy_last_response()*
                          `Sllm.copy_last_response`()
Copy the last response from the LLM buffer to the clipboard.
Return ~
`(nil)`

                                                          *Sllm.complete_code()*
                             `Sllm.complete_code`()
Complete code at cursor position.
Return ~
`(nil)`

                                                         *Sllm.browse_history()*
                            `Sllm.browse_history`()
Browse chat history, load a conversation, and continue from it.
Return ~
`(nil)`


 vim:tw=78:ts=8:noet:ft=help:norl: